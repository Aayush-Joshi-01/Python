{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "Cv0Tbz43GnWI",
        "outputId": "0107a5ee-3dcc-4f8d-822e-fd0072daad75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'sudo' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "The system cannot find the path specified.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open 'spark-3.2.1-bin-hadoop3.2.tgz'\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in d:\\python_training\\.venv\\lib\\site-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in d:\\python_training\\.venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py4j in d:\\python_training\\.venv\\lib\\site-packages (0.10.9.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://YISC1100673LT.Yash.com:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x25f7b9c1b80>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbOcBrv0HnhC"
      },
      "source": [
        "# Importing  Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r9Rz1cpzIAt7"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o28.addFile.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1795)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1733)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m chipo \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m SparkFiles\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchipotle.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m), sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Python_Training\\.venv\\Lib\\site-packages\\pyspark\\context.py:1898\u001b[0m, in \u001b[0;36mSparkContext.addFile\u001b[1;34m(self, path, recursive)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maddFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;124;03m    Add a file to be downloaded with this Spark job on every node.\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m \u001b[38;5;124;03m    The `path` passed can be either a local file, a file in HDFS\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;124;03m    [100, 200, 300, 400]\u001b[39;00m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python_Training\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32md:\\Python_Training\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32md:\\Python_Training\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.addFile.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1795)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1733)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "chipo = spark.read.csv(\"file://\" + SparkFiles.get(\"chipotle.tsv\"), sep='\\t', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaXArQlOJSg7"
      },
      "source": [
        "1. See the first 10 entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dusUpVh_I6CT",
        "outputId": "f2599ca6-ae1e-48f3-e7e0-222bd816d7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+--------------------+--------------------+----------+\n",
            "|order_id|quantity|           item_name|  choice_description|item_price|\n",
            "+--------+--------+--------------------+--------------------+----------+\n",
            "|       1|       1|Chips and Fresh T...|                NULL|    $2.39 |\n",
            "|       1|       1|                Izze|        [Clementine]|    $3.39 |\n",
            "|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |\n",
            "|       1|       1|Chips and Tomatil...|                NULL|    $2.39 |\n",
            "|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
            "|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
            "|       3|       1|       Side of Chips|                NULL|    $1.69 |\n",
            "|       4|       1|       Steak Burrito|[Tomatillo Red Ch...|   $11.75 |\n",
            "|       4|       1|    Steak Soft Tacos|[Tomatillo Green ...|    $9.25 |\n",
            "|       5|       1|       Steak Burrito|[Fresh Tomato Sal...|    $9.25 |\n",
            "+--------+--------+--------------------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akGBPlSMI_TH"
      },
      "source": [
        "2. What is the number of observations in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqJxN206Jbgx",
        "outputId": "b8e7f6f5-84bf-4ca3-c626-804ca538cdd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4622"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chipo.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyO5PgxKJeSe",
        "outputId": "4f57f6c3-3a8c-4fb8-960f-ce24d714da25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- item_name: string (nullable = true)\n",
            " |-- choice_description: string (nullable = true)\n",
            " |-- item_price: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOFHD6UyJq2M",
        "outputId": "660e3e36-16a8-455b-f932-6e734be2282f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4622"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chipo_pd = chipo.toPandas()\n",
        "len(chipo_pd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5RNFdedJ9sg"
      },
      "source": [
        "3. What is the number of columns in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyYf1DKbJy9f",
        "outputId": "ec4e2602-eaea-42e3-cb04-bca3735eec64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chipo.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kon9cvrQKGQd"
      },
      "source": [
        "4. Print the name of all the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWgTk46jKJSl",
        "outputId": "e8c5326d-8a00-4c7c-bc2e-69b0166422f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['order_id', 'quantity', 'item_name', 'choice_description', 'item_price']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chipo.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_J84VAFKPwo"
      },
      "source": [
        "5. How is the dataset indexed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ooIp2lgKmIM",
        "outputId": "0ac344c8-5584-475e-8134-4eb68a2f892b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+---+\n",
            "| id|order_id|quantity|           item_name|  choice_description|item_price| id|\n",
            "+---+--------+--------+--------------------+--------------------+----------+---+\n",
            "|  0|       1|       1|Chips and Fresh T...|                NULL|    $2.39 |  0|\n",
            "|  1|       1|       1|                Izze|        [Clementine]|    $3.39 |  1|\n",
            "|  2|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |  2|\n",
            "|  3|       1|       1|Chips and Tomatil...|                NULL|    $2.39 |  3|\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |  4|\n",
            "+---+--------+--------+--------------------+--------------------+----------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# By default, Spark DataFrames are not indexed in the same way as Pandas DataFrames.\n",
        "# Spark DataFrames are distributed collections of data, and they don't have an explicit index.\n",
        "\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "df = chipo.withColumn(\"id\", monotonically_increasing_id())\n",
        "df.select(\"id\", \"*\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gAEmYarLqwt"
      },
      "source": [
        "6. Which was the most-ordered item?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJSNRls3Lt11",
        "outputId": "7d7de6bc-1ef6-41c4-df43-1b24cc1b5bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "|   item_name|count|\n",
            "+------------+-----+\n",
            "|Chicken Bowl|  726|\n",
            "+------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.groupBy(\"item_name\").count().orderBy(\"count\", ascending=False).show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhQt6As7NVd0"
      },
      "source": [
        "7. For the most-ordered item, how many items were ordered?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urnFzvdVNZLv",
        "outputId": "3d1439f2-14dc-493b-e902-5fd333735045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "|   item_name|count|\n",
            "+------------+-----+\n",
            "|Chicken Bowl|  726|\n",
            "+------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.groupBy(\"item_name\").count().orderBy(\"count\", ascending=False).show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xb4-Uc7Nl07"
      },
      "source": [
        "8. What was the most ordered item in the choice_description column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4H5QX49Nne7",
        "outputId": "b6eed9b0-2616-4220-b98b-7ec3eb26babe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(choice_description='[Diet Coke]', count=134)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chipo.groupBy(\"choice_description\").count().orderBy(\"count\", ascending=False).collect()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvVhwWpIV7rW",
        "outputId": "59dd95a3-d941-4c91-df44-3c0c5e30fbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----+\n",
            "|choice_description|count|\n",
            "+------------------+-----+\n",
            "|              NULL| 1246|\n",
            "|       [Diet Coke]|  134|\n",
            "+------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.groupBy(\"choice_description\").count().orderBy(\"count\", ascending=False).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXVe5dkiVHww",
        "outputId": "66fc6f83-07c2-43be-e3a7-41fb9337cb32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------------+\n",
            "|choice_description|sum(quantity)|\n",
            "+------------------+-------------+\n",
            "|              NULL|         1382|\n",
            "|       [Diet Coke]|          159|\n",
            "+------------------+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.groupBy(\"choice_description\").sum(\"quantity\").orderBy(F.desc(\"sum(quantity)\")).show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xWjKftHOhSQ"
      },
      "source": [
        "9. How many items were orderd in total?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhAH3zW-ObsL",
        "outputId": "555985fa-7e8f-441f-d1ef-f705339a9115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|sum(quantity)|\n",
            "+-------------+\n",
            "|         4972|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.agg(F.sum(\"quantity\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkjQOR1bO88S"
      },
      "source": [
        "10. Turn the item price into a float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdIvZu6CPA4y",
        "outputId": "3caa56f9-dba5-43af-dd58-328baaa5dea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+--------------------+--------------------+----------+\n",
            "|order_id|quantity|           item_name|  choice_description|item_price|\n",
            "+--------+--------+--------------------+--------------------+----------+\n",
            "|       1|       1|Chips and Fresh T...|                NULL|      2.39|\n",
            "|       1|       1|                Izze|        [Clementine]|      3.39|\n",
            "|       1|       1|    Nantucket Nectar|             [Apple]|      3.39|\n",
            "|       1|       1|Chips and Tomatil...|                NULL|      2.39|\n",
            "|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|     16.98|\n",
            "+--------+--------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo = chipo.withColumn(\"item_price\", F.regexp_replace(\"item_price\", \"\\$\", \"\").cast(\"float\"))\n",
        "chipo.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx9WvqLePUPz"
      },
      "source": [
        "10. (a) Check the item price type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zpXVzxUPabc",
        "outputId": "76915a66-a0ba-4a82-ed55-a615ccddba4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- item_name: string (nullable = true)\n",
            " |-- choice_description: string (nullable = true)\n",
            " |-- item_price: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nWGyjg-QXOU",
        "outputId": "86938b6c-19c5-4ed1-90e6-2f70fed7864b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('item_price', 'float')]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chipo.select(\"item_price\").dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWDV0wDJQq9w"
      },
      "source": [
        "10. (b) Create a lambda function and change the type of item price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b93DttkiQxRA",
        "outputId": "dbd9a712-ddf1-470d-e24b-aad89e396cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- item_name: string (nullable = true)\n",
            " |-- choice_description: string (nullable = true)\n",
            " |-- item_price: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "change_type = F.udf(lambda x: float(x), StringType())\n",
        "chipo = chipo.withColumn(\"item_price\", change_type(chipo[\"item_price\"]))\n",
        "chipo.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIlG0cqsSucF"
      },
      "source": [
        "11. How much was the revenue for the period in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeOvGvrnSxgL",
        "outputId": "d2516e41-80d2-44c6-f03c-37c75153633f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+\n",
            "|sum(revenue_per_item)|\n",
            "+---------------------+\n",
            "|    39237.01973223686|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.withColumn(\"revenue_per_item\", chipo[\"item_price\"] * chipo[\"quantity\"]).agg(F.sum(\"revenue_per_item\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PryaV3hlTmQv"
      },
      "source": [
        "12. How many orders were made in the period?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGmpsbhGTqpI",
        "outputId": "ab860e81-96ff-458a-9991-f21796180604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------+\n",
            "|count(DISTINCT order_id)|\n",
            "+------------------------+\n",
            "|                    1834|\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.agg(F.countDistinct(\"order_id\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgqgVhuJT00H"
      },
      "source": [
        "13. What is the average revenue amount per order?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTCRed5MT8T5",
        "outputId": "b93aa262-bdda-4a4a-ca9e-eaed02743c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|avg(avg_revenue)|\n",
            "+----------------+\n",
            "|8.48918644141862|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.withColumn(\"avg_revenue\", chipo[\"item_price\"] *chipo[\"quantity\"]).agg(F.mean(\"avg_revenue\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYfiGpV1UD7P"
      },
      "source": [
        "14. How many different items are sold?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJErAE0eUJK4",
        "outputId": "c513db59-5c60-45c2-fb74-7a7f0c77bcf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------+\n",
            "|count(DISTINCT item_name)|\n",
            "+-------------------------+\n",
            "|                       50|\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chipo.agg(F.countDistinct(\"item_name\")).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
