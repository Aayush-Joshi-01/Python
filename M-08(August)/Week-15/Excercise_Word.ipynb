{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rStJPwACFlQd"
      },
      "outputs": [],
      "source": [
        "! pip install pyspark\n",
        "! pip install py4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "TNhzxRO7F1A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "ntD0BBQtF5d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read.csv('/content/employees.csv', header=True, inferSchema=True)\n",
        "emp_df.show()"
      ],
      "metadata": {
        "id": "c6Cp_uucHOJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df = spark.read.csv('/content/departments.csv', header=True, inferSchema=True)\n",
        "dept_df.show()"
      ],
      "metadata": {
        "id": "86_0Wr10HVjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emp_df.write.json('/content/employees')"
      ],
      "metadata": {
        "id": "KsRPrG63HpsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dept_df.write.json('/content/departments')"
      ],
      "metadata": {
        "id": "c1YdXCZRHvFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.columns"
      ],
      "metadata": {
        "id": "tka0FKhKIXJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df.columns"
      ],
      "metadata": {
        "id": "pByzDRMmIaWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tPerform a left  join between two DataFrames."
      ],
      "metadata": {
        "id": "73WcukqtK0xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, emp_df.DEPARTMENT_ID == dept_df.DEPARTMENT_ID, 'left').show()"
      ],
      "metadata": {
        "id": "OjYdglHIICZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tUse a window function to calculate the rank of employees based on their salaries within each department."
      ],
      "metadata": {
        "id": "aPMuKdSFKyiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "FvzGX51vIUph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec = Window.partitionBy(\"DEPARTMENT_ID\").orderBy(F.desc(\"SALARY\"))"
      ],
      "metadata": {
        "id": "ZWhbOL1ALYMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_df = emp_df.withColumn(\"rank\", F.rank().over(windowSpec))"
      ],
      "metadata": {
        "id": "WDOmK3afLZoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_df.orderBy(\"rank\").show()"
      ],
      "metadata": {
        "id": "73DNENcBLdaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tWrite a DataFrame to a Parquet file with partitioning by a specific column."
      ],
      "metadata": {
        "id": "lKqPXwdLkMbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# emp_df.write.partitionBy('DEPARTMENT_ID').parquet('/content/employees_parquet')"
      ],
      "metadata": {
        "id": "kyNDqRJAlF__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dept_df.write.partitionBy('DEPARTMENT_ID').parquet('/content/departments_parquet')"
      ],
      "metadata": {
        "id": "d57BH2gSmxhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Use the mapPartitions transformation to apply a function to each partition of the\n",
        "DataFrame."
      ],
      "metadata": {
        "id": "2nng6b_GELQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_partition(iterator):\n",
        "  for row in iterator:\n",
        "    print(row)\n",
        "    yield row\n",
        "result_rdd = emp_df.rdd.mapPartitions(process_partition)\n",
        "result = result_rdd.collect()\n",
        "for i in result:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "4BLlUJVEEHWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Implement a custom aggregation function using aggregateByKey."
      ],
      "metadata": {
        "id": "PkGLgdNzFEAa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYs4URKpERL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Perform a cross join between two DataFrames and filter the results based on a\n",
        "condition\n"
      ],
      "metadata": {
        "id": "90P7lDM8F1LN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMrqt_wCF4C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Use the flatMap transformation to split each element of a DataFrame column into\n",
        "multiple rows."
      ],
      "metadata": {
        "id": "NDnX0I-sF6Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "593NBvHEF6QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Create a DataFrame from an RDD and specify the schema manually"
      ],
      "metadata": {
        "id": "VhqVzKH-F8Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aAPrMUuF8nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Use the reduceByKey transformation to sum values by key in a DataFrame.\n"
      ],
      "metadata": {
        "id": "WcMLA5NjF-9V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnFw_JZBF_Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Perform a rolling average calculation using a window function."
      ],
      "metadata": {
        "id": "ysPz2PziGBTO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJOhba7pGBgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Use the foreachPartition action to write each partition of a DataFrame to a\n",
        "separate file."
      ],
      "metadata": {
        "id": "nJc6cLUCGDTx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1xGPSZsGDad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Implement a custom partitioner for an RDD and apply it to a DataFrame."
      ],
      "metadata": {
        "id": "osvQ4nWJGFPd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qD72ERMBGFev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Perform a union of two DataFrames with different schemas and handle the schema\n",
        "mismatch."
      ],
      "metadata": {
        "id": "DMdL8umPGIEO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "20vXoYbmGISf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Use the groupByKey transformation to group values by key and apply a custom\n",
        "aggregation function."
      ],
      "metadata": {
        "id": "ican1lUtGKkL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rddk6trZGKsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Perform a right outer join between two DataFrames and handle null values in the\n",
        "result."
      ],
      "metadata": {
        "id": "dpMrp_FyGMFA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmRZTm0UGMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Use the sampleBy transformation to perform stratified sampling on a DataFrame.\n"
      ],
      "metadata": {
        "id": "svnfsZs9GN8O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZeXqzMIFGOCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Implement a custom UDF to perform complex string manipulations on a DataFrame\n",
        "column."
      ],
      "metadata": {
        "id": "oXETfmnEGPbs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuyjOGCYGPmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}